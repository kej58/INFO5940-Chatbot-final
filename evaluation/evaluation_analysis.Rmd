---
title: "Prompt Evaluation & Analysis Report"
output:
  html_document:
    toc: true
    toc_depth: 2
    theme: united
    highlight: tango
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
library(readr)
library(dplyr)
library(knitr)
library(ggplot2)
library(reshape2)
```

## 1. Evaluation Dimensions

| Dimension | Description |
|---------------------|-------------|
| **Completeness**     | Whether the answer fully addresses the question with sufficient scope and depth. |
| **Relevance**        | How well the answer aligns with the prompt intent and phrasing. |
| **Authority**        | The credibility of cited sources (e.g., .gov, academic journals). |
| **Recency**          | How recent and up-to-date the cited information is. |
| **Factual Accuracy** | Whether the response includes accurate and verifiable facts. |
| **Information Depth**| The level of detail, mechanism, reasoning, or data involved in the answer. |

To evaluate RAG outputs more precisely, we used a six-dimension framework: completeness, relevance, authority, recency, factual accuracy, and information depth. This structure enabled more targeted and nuanced assessments beyond a single overall score, helping reveal not just what was said, but how well-supported and informative each response was.

## 2. Scoring Process and Summary Table
```{r}

scores <- read_csv("Prompt Evaluation Table.csv", show_col_types = FALSE)
kable(head(scores, 5), format = "markdown")
```


*Note: The table above shows the first 5 rows. Full evaluation scores are included in the CSV file.*

Each prompt output was scored across the six dimensions. To ensure fairness, we compared prompts within the same topic and used LLM tools to help convert qualitative judgments into consistent numerical scores. Human oversight was still applied for edge cases such as unclear sources. The resulting scores were aggregated by prompt type, and the summary table below shows the average for each dimension. This forms the basis for deeper analysis.

## 3. Prompt Type vs Score Analysis

To analyze how different prompt types performed, we calculated average scores across all six evaluation dimensions and visualized the results through a combination of summary table, heatmap, and bar chart. These tools provided both detailed and big-picture insights into performance trends.

### 3.1 Summary Table

The summary table below displays the average score for each evaluation dimension by prompt type. We grouped prompt outputs by type and calculated the mean for each dimension, including a final average across all dimensions. This table served as the foundation for further visual analysis.

```{r}
score_summary <- scores %>%
  group_by(Prompt_Type) %>%
  summarise(
    Completeness = round(mean(Completeness), 2),
    Relevance = round(mean(Relevance), 2),
    Authority = round(mean(Authority), 2),
    Recency = round(mean(Recency), 2),
    Factual_Accuracy = round(mean(Factual_Accuracy), 2),
    Information_Depth = round(mean(Information_Depth), 2),
    Average_Score = round(mean(Average_Score), 2)
  )

kable(score_summary)
```

### 3.2 Heatmap of Evaluation Dimensions

The heatmap provides a visual representation of how each prompt type performed across all six evaluation dimensions. Color intensity corresponds to the score magnitude, making it easy to compare strengths and weaknesses at a glance. This view helps identify prompt types that perform consistently well across dimensions and those that struggle in areas such as authority or depth.

```{r fig.width=12}
library(reshape2)
library(ggplot2)

score_long <- melt(score_summary, id.vars = "Prompt_Type")
ggplot(score_long, aes(x = variable, y = Prompt_Type, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "white", high = "steelblue") +
  labs(x = "Evaluation Dimension", y = "Prompt Type", fill = "Score") +
  theme_minimal()

```

### 3.3 Bar Chart of Overall Performance

The bar chart ranks prompt types based on their overall average score. By sorting from lowest to highest and displaying results horizontally, the chart clearly reveals which prompt styles are most effective overall. It complements the heatmap by offering a more direct comparison of general effectiveness across prompt categories.

```{r}
ggplot(score_summary, aes(x = reorder(Prompt_Type, Average_Score), y = Average_Score)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(title = "Average Score by Prompt Type", x = "Prompt Type", y = "Average Score") +
  theme_minimal()

```

Together, the table and charts form a cohesive analytical pipeline: the table provides quantitative structure, the heatmap reveals dimension-level contrasts, and the bar chart clarifies which prompt types are most effective overall. By combining multiple visual formats, I aimed to present both granular insights and big-picture takeaways in a way that would support the key findings that follow.


## 4. Key Findings

Our evaluation revealed several important patterns in how different prompt types performed across six quality dimensions. These findings are based on the averaged scores shown in the summary table, heatmap, and bar chart.

### Top Performers

- **Comparative – Clinical Approaches** achieved a perfect score of **5.00** across all six dimensions, making it the highest-performing prompt type overall. It demonstrated exemplary completeness, authority, and depth.
- Other **comparative prompt types**—including **Practice Gap**, **Cultural**, and **Applications**—consistently scored above **4.7**, showing strong results particularly in **Authority**, **Information Depth**, and **Factual Accuracy**.

### Contextual Prompts

- **Context – Technical** prompts had an average score of **4.78**, with especially high marks for **Completeness**, **Information Depth**, and **Factual Accuracy**. This suggests technical framing can prompt the model to produce detailed, expert-level responses.
- **Context – Personal** prompts also performed well (**4.72** average), particularly excelling in **Relevance** and **Factual Accuracy**, likely due to their grounding in real-world scenarios.

### Lower-Performing Prompt Types

- **Basic prompts** (Simple, Specific, Background) generally scored lower across several dimensions:
  - While **Completeness** and **Relevance** remained high (≥ 4.67), these prompt types often lacked **Authority** and **Information Depth**, leading to average scores ranging from **4.22** to **4.59**.
  - **Basic – Specific** was the lowest-performing prompt type overall (**4.22**), with particularly low **Authority** (3.00) and **Information Depth** (4.00).

- **Comparative – Geographic** underperformed relative to other comparative prompts, with a lower **Authority score (3.00)** and overall average of **4.25**. This may reflect the difficulty of retrieving consistent, high-quality international comparisons or lack of specific regional data in retrieved sources.

### General Observations

- The **heatmap** made it clear that **Comparative** and **Technical** prompts had consistent strengths across all dimensions, whereas **Basic prompts** showed uneven performance—strong in Relevance but weak in source reliability and depth.
- The **bar chart** further confirmed that prompt structure significantly impacts RAG performance. Prompts that introduced comparative reasoning or technical framing encouraged the model to retrieve more authoritative, richer responses.